{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_clustering.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOKvHejPs/DEAIZf8o1ejR8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GPHesUYrYAWL"},"source":["# **Clustering**\r\n","Also know as *unsupervised classification*.\r\n","\r\n","* **Given**: a set of $N$, each described by $D$ values (dimension);\r\n","* **Task**: find a *natural partitioning* in $K$ cluster and possibly a number of *noise objects*;\r\n","* **Result**: a *clustering scheme* (i.e. function mapping each data object to a sequence $[1,\\dots,K]$).\r\n","\r\n","Note that:\r\n","* Objects in the same cluster are similar $\\to$ Maximize intra-cluster similarity;\r\n","* Objects in different clusters are not similar $\\to$ Minimize inter-cluster similarity.\r\n","\r\n","Type of methods:\r\n","* Partitionig (i.e. $K$-means, expectation maximization, CLARANS);\r\n","* Hierarchic (i.e. agglomerative/divisive, BIRCH, CURE);\r\n","* Based on linkage;\r\n","* Based on density (i.e DBSCAN, DENCLUE);\r\n","* Statistics (i.e. IBM-IM, COBWEB, Autoclass)."]},{"cell_type":"markdown","metadata":{"id":"VAlAjqgUYKu5"},"source":["## $K$-means\r\n","Let's have some data that could be modeled as a five component gaussian mixture, how do we guess the number of cluster in a $D$-dimensional space (with $D \\ge 2$).\r\n","\r\n","Let's pretend we have to transmit this information, allowing only two bits per point (lossy transmission), then we'll need a encoding/decoding mechanism.\r\n","\r\n","**How much is this loss?** \r\n","* First idea: let's try with the squared errors between reals point and their encoding/decoding, considering a partition of the space into a grid of cells (decode each pairs of bit with the center of the grid cell, midpoint);\r\n","* An improvement: instead of the midpoint use the centroid of the points in the grid cell (vectorial summation divided by the number of points);\r\n","* $K$-means: ask the user the number of clusters $K$ (a bit like cheating) with a random choice of $K$ points as temporary centers. Each point finds his nearest center and for each center finds the centroid of its points (compute the centroid) and move there the center (**iteratively**). $K$-means ends with a stable solution.\r\n","\r\n","Some questions:\r\n","* What are we trying to optimize? Distortion.\r\n","* Is termination guaranteed? Nope, sometimes the algorithm can loop.\r\n","* The best clustering scheme is find? What is the definition of best clustering scheme? Depends on the application.\r\n","* How should we start? Randomly.\r\n","* How can we find the number of clusters? Try and improvement.\r\n"]},{"cell_type":"markdown","metadata":{"id":"x02NrbK_efCN"},"source":["### Distortion\r\n","Given:\r\n","* A encoding function: $\\text{Encode}:\\mathbb{R}^D\\to[1,\\dots,K]$;\r\n","* A decoding function: $\\text{Decode}:[1,\\dots,K]\\to\\mathbb{R}^D$;\r\n","* Define: $\\text{Distortion}=\\sum_{i=1}^N(e_i-\\text{Decode}(\\text{Encode}(e_i)))^2$;\r\n","* A shortcut: $\\text{Decode}(k)=c_k$ (code of $k$, centers).\r\n","\r\n","Then we can define: $$\\text{Distortion}=\\sum_{i=1}^N(e_i-c_{\\text{Encode}(e_i)})^2$$\r\n","\r\n","To have minimal distortion $e_i$ must:\r\n","1. Be encoded with the nearest center (otherwise the distortion could be reduced by substituting $\\text{Encode}(e_i)$ with the nearest center): $$c_{\\text{Encode}(e_i)}=\\underset{c_j\\in\\{c_1,\\dots,c_k\\}}{\\operatorname{argmin}}(e_i-c_j)^2$$\r\n","1. The partial derivative of distortion w.r.t. the position of each center must be zero (minimization $\\nabla f=0$). When distortion is minimal: $$c_j=\\frac{1}{|\\text{OwnedBy}(c_j)|}\\sum_{i \\in \\text{OwnedBy}(c_j)}(e_i-c_j)$$ \r\n","3. Each center must be the centroid of the points it owns.\r\n","\r\n","Algorithm: for improving a sub-optimal solution alternately steps 1 and 3. After a finite number of steps the system reaches a state where neither of the two operations changes the state (the distortion function is strictly convex)."]},{"cell_type":"markdown","metadata":{"id":"5YqKENtNj5AQ"},"source":["### Algorithm termination\r\n","There's only a finite number of ways to partition $N$ objects into $K$ gropus.\r\n","The state of the algorithm is given by the two coding functions.\r\n","\r\n","The number of configurations where all the centers are centroids of the points they own is finite.\r\n","\r\n","If after an iteration the state changes, the distortion is reduced (since it's a convex function). \r\n","Each change of state bring to a state which was never visited before.\r\n","Sooner or later the algorithm will stop because there are no new states reachable.\r\n","\r\n","The ending state is not necessarily the best possible.\r\n","\r\n","The starting point is important, usually: choose randomly the first starting point and then choose in sequence the $[2,\\dots,K]$ as far as possible from the preceding ones.\r\n","\r\n","Note: usually starting with different random states is enough to obtain a very good solution."]},{"cell_type":"markdown","metadata":{"id":"iLKsO1tlmaBN"},"source":["### Choosing the number of clusters\r\n","There are different methods:\r\n","* Try various values ($k$ is an hyperparameter);\r\n","* Use quantitative evaluation of the quality of the clustering sccheme to decide among the different values;\r\n","* The best value is an optimal compromise between the minimization of the intra-cluster distances and the maximization of the inter-cluster distances (need to measure them, not so trivial).\r\n"]},{"cell_type":"markdown","metadata":{"id":"glnZK9bInQnf"},"source":["### Proximity function\r\n","The choice of the function is completely independent from the algorithm.\r\n","\r\n","**Distortion (or inertia) a.k.a. sum of squared error (SSE)**: $$\\text{SSE}=\\sum_{j=1}^K \\sum_{i \\in \\text{OwnedBy}(c_j)}(e_i-c_j)^2=\\sum_{j=1}^K\\text{SSE}_j$$\r\n","\r\n","* A cluster $j$ with an high $\\text{SSE}_j$ has a low quality;\r\n","* If $\\text{SSE}_j=0$ iff points are coincident with the centroid;\r\n","* $\\text{SSE}$ decreases for increasing $K$, is zero when $K=N$.\r\n","\r\n","In general minimizing $\\text{SSE}$ is not a viable solution to choose the best $K$."]},{"cell_type":"markdown","metadata":{"id":"D1lKPK98o_xT"},"source":["### Empty clusters\r\n","It may happen that at some step a centroid doesn't own any point.\r\n","This would changes the initial requirement of $K$ clusters, so choose a new centroid:\r\n","* Choose a point far away from the empty centroid;\r\n","* Or choose as new centroid a random point in the cluster with the maximum $\\text{SSE}$ (worst cluster). In this way the cluster with the lowest quality will be split in two."]},{"cell_type":"markdown","metadata":{"id":"k47_rMSYpwBi"},"source":["### Outliers\r\n","Outliers are point with high distance from their centroid, they give an high contribution to the $\\text{SSE}$, and so the could have a bad influence on the clustering scheme (sometimes is a good idea to remove them)."]},{"cell_type":"markdown","metadata":{"id":"wJe89X0RqJfs"},"source":["### Common uses of $K$-means\r\n","* Can be easily used in the beginnign for data exploration (it's very fast);\r\n","* In a one-dimensional space it's a good way to discretize the values of a domain in non-uniform buckets;\r\n","* It's the basis for vector quantization, a technique for signal processing and compression or used for choosing the color palettes (color quantization)."]},{"cell_type":"markdown","metadata":{"id":"Re5RDTG2q1tT"},"source":["### Complexity\r\n","The time complexity is $\\mathcal{O}(TKND)$, with $T$ iterations, $K$ clusters and $N$ datapoints with $D$ dimensions."]},{"cell_type":"markdown","metadata":{"id":"5ylonFutrXK_"},"source":["## Evaluation of a clustering scheme\r\n","*Related to result, indipendent from clustering technique.*\r\n","\r\n","For example clustering is an unsupervised method, no has ground truth to compare the result (basically no apriori information), so we need indexes to measure various properties of the clusters and the clustering scheme as a whole.\r\n","If some supervised data are available they can be used to evaluate the clustering scheme.\r\n","\r\n","In $2$-dimensional spaces, clusters can be examined visually; in higher order spaces projections can help but formal methods are better.\r\n","\r\n","The general objective is to distinguish patterns from random apparent regularities and find the best number of clusters.\r\n","\r\n","The main measurement criteria are:\r\n","* **Cohesion** (to maximize): proximity of objects in the same cluster should be high;\r\n","* **Separation** (to minimize): distance between the nearest objects in the two clusters, distance between the most distance objects in the two clusters, distance between the centroids, etc... \r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"jE1Hxw43t0kq"},"source":["### Cohesion and separation\r\n","**Cohesion** - it's the sum of proximities between the elements of the cluster and the geometric center (prototype):\r\n","* **Centroid**: a point in the space whose coordinates are the means of the dataset;\r\n","* **Medoid**: an element of the dataset whose average dissimilarity with all the elements of the cluster is minimal. Used in contexts where means is not defined.\r\n","$$\\text{Coh}(k_i)=\\sum_{x \\in k_i}\\text{Prox}(x,c_i)$$\r\n","\r\n","**Separation** - proximity between the prototypes (centroid or medoids):\r\n","$$\\text{Sep}(k_i,k_j)=\\text{Prox}(c_i,c_j)$$\r\n","\r\n","![](https://i.ibb.co/47JM8rv/aegvevr.png)\r\n","\r\n","**Global separation of a clustering scheme**: bigger clusters will have a bigger influence. The sum of squares between clusters is defined as:\r\n","$$\\text{SSB}=\\sum_{i=1}^K N_i \\text{Dist}(c_i,c)^2$$\r\n","\r\n","Let be $\\text{TSS}$ the total sum of squares (sum of squared distances of the point from the global centroid), then $\\text{TSS}=\\text{SSE}+\\text{SSB}$.\r\n","Since the sum is fixed then if we increase one the other decreases.\r\n","\r\n","Note: $\\text{TSS}$ is a global property of the dataset, independent from the clustering scheme."]},{"cell_type":"markdown","metadata":{"id":"yNpyHNQJxLyt"},"source":["### Evaluation\r\n","Each cluster can have its own evaluation: worst clusters can be considered for additional split and weakly separated pairs of clusters can be considere for merging.\r\n","\r\n","Single objects can give negative contribution to the cohesion of a\r\n","cluster or to the separation between two clusters (border objects).\r\n"]},{"cell_type":"markdown","metadata":{"id":"xDeeQqauxhai"},"source":["### [Silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)) index of a cluster\r\n","Computation:\r\n","* For the $i$-th object inside the cluster, let be the the mean distance between $i$ and all other data points in the same cluster. Let's call it $a_i$.We can interpret $a(i)$ as a measure of how well $i$ is assigned to its cluster (the smaller the value, the better the assignment).\r\n","* For the $i$-th object and for each cluster other than its own, compute the mean distance of $i$ to all points in any other cluster, of which $i$ is not a member. Let's call it $b_i$.\r\n","\r\n","For the $i$-th object the silhouette index is:\r\n","$$s_i=\\frac{b_i-a_i}{\\max(a_i,b_i)}\\in [-1,1]$$\r\n","\r\n","For the global index of a *cluster or clustering scheme* compute the average index over the *cluster or dataset*.\r\n","\r\n","When the index is less than zero for an object, it means that there is a dominance of objects in other clusters at a distance smaller than the objects of the same cluster (well inside the cluster)."]},{"cell_type":"markdown","metadata":{"id":"2ezA9Rk0zyCf"},"source":["### Looking for the best number of clusters\r\n","![](https://i.ibb.co/j8hgnTg/photo-2021-01-03-11-34-36.jpg)"]},{"cell_type":"markdown","metadata":{"id":"19D8J8d00teq"},"source":["### Supervised measures\r\n","* Let's be available a labelled partition $P=\\{P_1, \\dots, P_L\\}$ called **gold standard**;\r\n","* Consider the clustering scheme $K=\\{k_1,\\dots,k_K\\}$\r\n","\r\n","We want to compare them to validate the clustering techinque.\r\n","We have two main ways:\r\n","* **Classification oriented**: measure how the classes are distribuited among the clusters (confusion matrix, precision, recall, F-measure, etc...);\r\n","* **Similarity oriented**: analogous to binary data (rand index, Jaccard coefficient, etc...)."]},{"cell_type":"markdown","metadata":{"id":"dtaN_OOy1wz5"},"source":["## Hierarchical clustering\r\n","It generates a nested structure of clusters that can be:\r\n","* **Agglomerative**: as a starting state each datapoint is a cluster. In each step the two less separated clusters are merged into one. \r\n","A measure of separation between cluster is needed.\r\n","* **Divisive**: as a starting state the entire dataset is the only cluster.\r\n","In each step the cluster with the lowest cohesion is split.\r\n","A measure of cluster cohesion and a split procedure are needed.\r\n","\r\n","The distance between sets is based on the distances between objects belonging to the two sets."]},{"cell_type":"markdown","metadata":{"id":"SKUxrGAh4UGU"},"source":["### Graph based separation between clusters\r\n","![](https://i.ibb.co/Dkfcx3g/photo-2021-01-03-11-50-58.jpg)"]},{"cell_type":"markdown","metadata":{"id":"asDQ78D54Zkc"},"source":["### Prototype based separation between clusters\r\n","There are several alternatives:\r\n","* Distance between the centroids;\r\n","* **Ward's method** (the most intuitive): given two sets with the respective $\\text{SSE}$, the separation between the two is measured as the difference between the total $\\text{SSE}$ resulting in case of merge and the sum of the original $\\text{SSE}$.\r\n","Smaller separation implies a lower increase in the $\\text{SSE}$ after merging (so if the difference doesn't increase a lot it wasn't very separated).\r\n","\r\n","Note: when we merge the $\\text{SSE}$ increases."]},{"cell_type":"markdown","metadata":{"id":"B2rC5E445XZ9"},"source":["### Single linkage algorithm\r\n","1. Initialize the clusters, one for each objects;\r\n","2. Compute the distance matrix between the clusters (squared, symmetric, null main diagonal and size with the number of objects $N$);\r\n","3. While the number of clusters is greater than $1$:\r\n","> * Find the two clusters with lowest separation, say $k_r$ and $k_s$;\r\n","  * Merge them in a cluster;\r\n","  * Delete from the distance matrix row $r$ and column $s$ and insert a new row and column with the distances of the new cluster from the others: $$\\text{dist}\\bigl(k_k, k_{(r+s)}\\bigr) = \\min \\Bigl( \\text{dist}\\bigl(k_k, k_r), \\text{dist}\\bigl(k_k, k_s\\bigr) \\Bigr)$$\r\n","  $$\\forall k \\in [1,K]$$"]},{"cell_type":"markdown","metadata":{"id":"ObTmGXdy7QBA"},"source":["### Time and space complexity\r\n","![](https://i.ibb.co/DkmwvSh/comple.png)"]},{"cell_type":"markdown","metadata":{"id":"Plu4auYU78TM"},"source":["### Generating the clustering scheme\r\n","The desidered clustering scheme is obtained by cutting the obtained dendogram at asome level: the choice is application-dependent.\r\n","![](https://i.ibb.co/cxPHY7C/photo-2021-01-03-12-10-22.jpg)\r\n","The horizontal axis is the **total dissimilarity** inside the clusters, which (obviously) increases for decreasing number of clusters.\r\n","\r\n","The diameter of a cluster is the distance among the most separated objects:\r\n","* **Single linkage** tends to generate large clusters also at low level;\r\n","* **Complete linkage** tends to generate more compact clusters."]},{"cell_type":"markdown","metadata":{"id":"9n6ujC1a9Cus"},"source":["## Density based clustering\r\n","Clusters are high-density regions separated by low-density regions (not circular in general).\r\n","There are two main way to compute density:\r\n","* **Grid-based**: split the hyperspace into a regularly spaced grid and count the number of objects inside each grid element (hyperparameter: size of the grid);\r\n","* **Object-centered**: define the radius of an hypersphere and attach to each object the number of object which are inside that sphere (hyperparameter: radius)."]},{"cell_type":"markdown","metadata":{"id":"JVSoMwlw9_M_"},"source":["### DBSCAN - Density based spatial clustering application with noise\r\n","**Intuition**: $p$ is a border point, $q$ is a core point.\r\n","![](https://i.ibb.co/yXyxd2f/eragvaer.png)\r\n","\r\n","**Neighbourhood**: define a radius $\\varepsilon$ and define a neighbourhood of a point the $\\varepsilon$-hyperspere centered at that point. Points $p$ and $q$ are one in the neighbourhood of the other (the neighbourhood is symmetric).\r\n","![](https://i.ibb.co/nD59qV5/wscfasv.png)\r\n","\r\n","**Direct density reachability**: define a threshold $\\text{minPoints}$ and define as a **core** a point with at least $\\text{minPoints}$ points in its neighbourhood, or as a **border** otherwise.\r\n","\r\n","A point $p$ is **directly density reachable** from point $q$ iff: \r\n","* $q$ is a core;\r\n","* $q$ is in the neighbourhood of $p$.\r\n","\r\n","This property is not symmetric.\r\n","\r\n","![](https://i.ibb.co/0svjgvS/one.png)\r\n","\r\n","A point $p$ is **density reachable** from point $q$ iff: \r\n","* $q$ is a core;\r\n","* There is a sequence of points $q_i$ such that $q_{i+1}$ is directly density reachable from $q_i \\forall i \\in [1,nq]$;\r\n","* $q_1$ is directly reachable from $q$; \r\n","* $p$ is directly reachable from $q_{nq}$. \r\n","\r\n","This property is not symmetric.\r\n","\r\n","![](https://i.ibb.co/NsvG8PK/two.png)\r\n","\r\n","A point $p$ is **density connected** to point $q$ iff:\r\n","* There is a point $s$ such that $p$ and $q$ are density reachable from $s$.\r\n","\r\n","This property is symmetric.\r\n","\r\n","![](https://ibb.co/THvZzvH)\r\n","\r\n","**Generation of clusters**: a cluster is a maximal set of points connected by *density*. Border point which are not connected by density to any core point are labelled as noise.\r\n","\r\n","This method:\r\n","* Finds clusters of any shape;\r\n","* Is robust to noise;\r\n","* There are problem if the densities of clusters vary widely (it's distance-based $\\mathcal{O}(N^2)$)."]},{"cell_type":"markdown","metadata":{"id":"-UkAESxHZIFq"},"source":["### KDE - Kernel density estimation\r\n","A technique developed in statistics and pattern mining. \r\n","It's not based on distances but in neighbourhoods but on function distributions that describe the data.\r\n","\r\n","The overall density function is the sum of the **influence or kernel functions** associated with each point.\r\n","The kernel function must be symmetric and monotonically decreasing and usually has a parameter to set the decreasing rate."]},{"cell_type":"markdown","metadata":{"id":"2Eg0nQvYZ761"},"source":["### DENCLUE algorithm - Density clustering algorithm\r\n","1. Derive a density function for the space occupied by the data points:\r\n","1. Identify the point that are local maxima;\r\n","1. Associate each point with a **density attractor** by moving in the directions of maximum increase in density;\r\n","1. Define clusters consisting of points associated with a particular density attractor;\r\n","1. Discard clusters whose density attractor has a density less than a user-specified threshold $\\xi$;\r\n","1. Combine clusters that are connected by a path of points that all have a density of $\\xi$ or higher.\r\n","\r\n","The algorithm has a strong theoretical foundation (DBSCAN is a special case of DENCLUE where the density attractor is a step function).\r\n","It's good at dealing with the noise and cluster with different shapes and size but have some troubles with higher dimensional data and clusters with different densities.\r\n","Expensive computation $\\mathcal{O}(N^2)$.\r\n"]},{"cell_type":"markdown","metadata":{"id":"AscsIzk_b3a-"},"source":["## Model based clustering\r\n","Estimate the parameters of a statistical model to maximize the ability of the model to explain the data.\r\n","The main techinque is to use the mixture models: view the data as a set of observation from a mixture of different probability distributions.\r\n","Usually the base model is a multivariate normal.\r\n","\r\n","The estimation is usually done using the maximum likelihood: given a set of data $\\varepsilon$, the probability of the data is called likelihood function.\r\n","\r\n","Hypotesis: attributes are independent."]},{"cell_type":"markdown","metadata":{"id":"KWPCHZ6kc-dt"},"source":["### EM - Expectation maximization\r\n","If the data can be approximated by a single distribution the derivation of the parameters is straightforward.\r\n","In the general case, with many mixed distributions, the EM algorithm is used.\r\n","![](https://i.ibb.co/h9ttZz5/algo.jpg)"]},{"cell_type":"markdown","metadata":{"id":"nEaifXU0dtHm"},"source":["## Recap\r\n","* **Partitioning**: iteratively find partitions in the dataset optimizing some quality criterion (i.e. $k$-means);\r\n","* **Hierarchic**: recursively compute a structured hierarchy of subsets;\r\n","* **Density based**: compute metrics and aggregates clusters in high density areas;\r\n","* **Model based**: assume a model for the distribution and find the model parameters that guarantee the best fitting to the data.\r\n","\r\n","Effectiveness decreases with dimensionality $D$ and noise level.\r\n","\r\n","Computational cost increases with dataset size $N$ and dimensionality $D$."]}]}
