{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_association_rules.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3sur9Jovapv"
      },
      "source": [
        "# **Association rules**\r\n",
        "Given a set of commercial transactions, find rules that will predict the occurence of an item based on the occurences of other items in the transaction (discovering co-occurencies).\r\n",
        "\r\n",
        "Example: $\\{ \\text{Bread, Milk} \\} \\to \\{ \\text{Coke, Eggs} \\}$\r\n",
        "\r\n",
        "In this notation implication means co-occurencies, not causality.\r\n",
        "It's different from the boolean implication, it can be true with some level of truth (*fuzzy logic*).\r\n",
        "\r\n",
        "With $N$ item it's possible to derive $3^N$ association rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5LNT5Wvywa-"
      },
      "source": [
        "## Support and confidence\r\n",
        "*Our example:*\r\n",
        "\r\n",
        "![](https://i.ibb.co/rssRpwZ/toy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WDsxb3a0jfm"
      },
      "source": [
        "### Glossary\r\n",
        "**Itemset**: a collection of one or more items, i.e. $\\{ \\text{Milk, Bread Diaper} \\}$;\r\n",
        "\r\n",
        "**$k$-itemset**: an itemset with $k$ items;\r\n",
        "\r\n",
        "**Support count ($\\sigma$)**: frequency of occurence of an itemset, i.e. $\\sigma (\\{ \\text{Milk, Bread Diaper} \\}) = 2$;\r\n",
        "\r\n",
        "**Support**: fraction that transaction that contain an itemset, i.e. $\\sigma (\\{ \\text{Milk, Bread Diaper} \\}) = \\frac{2}{5}$;\r\n",
        "\r\n",
        "**Frequent itemset**: an itemset whose support is greater than or equal to a $\\text{minsup}$ threshold.\r\n",
        "\r\n",
        "**Association rule**: an expression of the form $A \\to C$, where $A$ and $C$ are itemsets (antecedent and consequent);\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQksw9bd2bme"
      },
      "source": [
        "### Rule evaluation metrics\r\n",
        "We need more than one since different transactions can have an equal metric.\r\n",
        "* Support: fraction of the $N$ transactions that contain both $A$ and $C$. Can decrease but not increase. \r\n",
        "$$\\text{sup} = \\frac{\\sigma(A+C)}{N}$$\r\n",
        "\r\n",
        "* Confidence: measure how often all the items in $C$ appear in transactions that contain $A$\r\n",
        "$$\\text{conf} = \\frac{\\sigma(A+C)}{\\sigma(A)}$$\r\n",
        "\r\n",
        "Rules with low support can be generated by random associations. Rules with low confidence are not really reliable.\r\n",
        "\r\n",
        "A rule with a relatively low support but high confidence can represent an uncommon but interesting phenomenon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYq3NI6C3Ehf"
      },
      "source": [
        "### Association rule mining\r\n",
        "Given a set of $N$ transactions, the goal of association rule mining is to find all rules having:\r\n",
        "* $\\text{sup} \\ge \\text{minsup}$;\r\n",
        "* $\\text{conf} \\ge \\text{minconf}$.\r\n",
        "\r\n",
        "1. **Brute-force** approach: list all possible association rules, compute support and confidence and prune the ones that fail the thresholding. Computationally prohibitive.\r\n",
        "\r\n",
        "*Note*: since rules originating from the same itemset have identical support but can have different confidence it's possible to decouple the requirements.\r\n",
        "\r\n",
        "2. **Two-step** approach:\r\n",
        "> * Step 1 - **Frequent itemset generation**: generate all the itemsets whose support is greater than the threshold. Computationally expensive;\r\n",
        "  * Step 2 - **Rule generation**: generate high confidence rules from each frequent dataset, where each rule is a binary partitioning of a frequent itemset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_agH5Fv95DEU"
      },
      "source": [
        "## Frequent itemset generation\r\n",
        "Given $D$ items there are $M=2^D$ possible candidates itemsets (two are empty and universe).\r\n",
        "\r\n",
        "![](https://i.ibb.co/26CwZsS/photo-2021-01-06-09-52-31.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzLo0Prn5rCP"
      },
      "source": [
        "### Brute-force approach\r\n",
        "1. Each itemset in the lattice is a candidate frequent itemset;\r\n",
        "1. Count the support of each candidate by scanning the database;\r\n",
        "1. Match each transaction against every candidate.\r\n",
        "\r\n",
        "The complexity is $\\mathcal{O}(NWM)$, being $W$ the average width of the transaction.\r\n",
        "\r\n",
        "Given $D$ unique items, the total number of itemsets is $2^D$ and the total number of association rules is:\r\n",
        "$$R = \\sum_{k = 1}^{D - 1} \\Biggr( {{D}\\choose{k}} \\times \\sum_{j = 1}^{D - k} {{D - k}\\choose{j}} \\Biggl) = 3^D - 2^{D + 1} + 1$$\r\n",
        "\r\n",
        "![](https://i.ibb.co/QcrVKDv/photo-2021-01-06-10-01-16.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKAE745S7uTE"
      },
      "source": [
        "### Pruning strategies\r\n",
        "1. Reduce the number of $M$ candidates (apriori principle);\r\n",
        "1. Reduce the number of $NM$ comparisons, using efficient data structures to store the candidates or transactions.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRvEiiKi8Pw7"
      },
      "source": [
        "### Apriori principle\r\n",
        "If an itemset is frequent, then all of its subsets must also be frequent. It holds due to the property of the support measure:\r\n",
        "$$\\forall X,Y \\colon (X \\subseteq Y) \\to \\text{sup}(X) \\ge \\text{sup}(Y)$$\r\n",
        "\r\n",
        "The support of an itemset never exceeds the support of its subsets (anti-monotone property of support).\r\n",
        "\r\n",
        "![](https://i.ibb.co/7vjJhkW/photo-2021-01-06-10-10-07.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2h-cuLw-eGA"
      },
      "source": [
        "#### Candidate generation\r\n",
        "Let be:\r\n",
        "* $C_k$ candidate itemsets of size $k$;\r\n",
        "* $L_k$ frequent itemsets of size $k$ (survivors);\r\n",
        "* $\\text{subset}_k(c)$ set of subsets of $c$ with $k$ elements.\r\n",
        "\r\n",
        "**Join step**:\r\n",
        "1. Let $L_k$ be represented as a table with $k$ columns where each row is a frequent itemset;\r\n",
        "1. Let the items in each row of $L_k$ be in lexicographic order;\r\n",
        "1. $C_{k+1}$ is generated by a self join of $L_k$ (just add a piece at the end).\r\n",
        "\r\n",
        "**Prune step**: each $(k+1)$-itemset which includes a $k$-itemset which is not in $L_k$ is deleted from $C_{k+1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqHUEiDTAQci"
      },
      "source": [
        "#### Frequent itemset generation\r\n",
        "![](https://i.ibb.co/thHtgV6/Cattura.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4AJDA8FAT5w"
      },
      "source": [
        "#### Example\r\n",
        "![](https://i.ibb.co/p2jzW5N/example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9seAUyjuAwoW"
      },
      "source": [
        "#### Complexity\r\n",
        "The computation is level-wise, the level is the cardinality of the itemsets under evaluation. \r\n",
        "The evaluation at level $k$ uses the apriori knowledge acquired from the previous levels, to reduce the search space.\r\n",
        "\r\n",
        "* **Choice of the minimum support threshold**: lowering the threshold results in a greater number of frequent itemsets. This may reduce pruning and increase the maximum lenght of frequent itemsets. The number of complete reads of the dataset is given by the maximum lenght of frequent itemsets plus one.\r\n",
        "\r\n",
        "* **Dimensionality of the dataset**: more space is needed to store support count of each item. If the number of frequent items also increases, both computation and I/O costs can increase.\r\n",
        "\r\n",
        "* **Size of database**: since apriori algorithm makes multiple passes, run time of the algorithm may increase with number of transactions.\r\n",
        "\r\n",
        "* **Average transaction width**: transaction width increases with denser dataset. This may increase the maximum lenght of frequent itemsets and traversals data structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLGrd-FCgcb"
      },
      "source": [
        "### Compact representation of frequent itemsets\r\n",
        "Even after filtering on support, the number of frequent itemsets can be very large.\r\n",
        "It's can be useful to identify a small representative of frequent itemsets\r\n",
        "\r\n",
        "* **Maximal frequent itemsets** - the smallest set of itemsets from which the frequent itemsets can be derived: their supersets are not frequent;\r\n",
        "\r\n",
        "* **Closed itemsets** - minimal representation of itemsets without losing support information: some itemsets are redundant because they have identical suppport as their supersets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSR9ZH0_ERa5"
      },
      "source": [
        "#### Maximal frequent itemset\r\n",
        "A **maximal frequent itemset** (MFI) doesn't have any frequent immediate supersets. They're neare the border dividing frequent by not frequent itemsets in the lattice.\r\n",
        "\r\n",
        "![](https://i.ibb.co/TLwKR8r/egvqaee.png)\r\n",
        "\r\n",
        "There exist efficient algorithms to explicitly find them without the enumeration of their subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B43_Jq_Eo8x"
      },
      "source": [
        "#### Closed itemsets\r\n",
        "An itemset is closed if none of its immediate supersets has the same support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBCUPN2QFdYa"
      },
      "source": [
        "#### Closed frequent itemsets\r\n",
        "$X \\to Y$ is redundant if there exists $X' \\to Y'$ such thtat the support and the confidence are the same and $X \\subseteq X'$ and $Y \\subseteq Y'$ (equality can't be in both).\r\n",
        "\r\n",
        "There exist efficient algorithms for efficient computation of closed frequent itemsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD7mtnypGc6o"
      },
      "source": [
        "## [FP-growth](https://www.softwaretestinghelp.com/fp-growth-algorithm-data-mining/)\r\n",
        "The problem with apriori algorithm is that it needs to generate the candidate itemsets (can be very high) and needs multiple scans of the database to check the support of the candidates.\r\n",
        "\r\n",
        "The **FP-growth** algorithm transforms the problem of *finding long frequent patterns* into *looking for shorter ones and the concatenating the suffix*.\r\n",
        "\r\n",
        "Uses a compressed representation of the database using a so called FP-tree and once that the three is constructed it uses a **recursive divide-and-conquer** approach to mine frequent itemsets.\r\n",
        "\r\n",
        "**[Steps](https://www.geeksforgeeks.org/ml-frequent-pattern-growth-algorithm/)**\r\n",
        "1. Scan the database to find support of 1-itemsets;\r\n",
        "1. Create the root for the FP-tree;\r\n",
        "1. Scan the database and for each transaction:\r\n",
        "> * Reorder the items for descending support;\r\n",
        "  * Focus on the root node of the FP-tree;\r\n",
        "  * For each item in the transaction: \r\n",
        "  >> * If it matches on of the descendants of the current node, move it and increment the count; \r\n",
        "    * Else create a new descendant node with count $1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7uHBbhQK7ee"
      },
      "source": [
        "### Conditional pattern base\r\n",
        "A *subdatabase* which consists of the set of frequent items co-occuring with the suffix pattern.\r\n",
        "Given a transaction database DB and a support threshold, the complete set of frequent item projections of transaction in the database can be derived from the respective FP-tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1FcNhiiLvia"
      },
      "source": [
        "### Efficiency\r\n",
        "\r\n",
        "![](https://i.ibb.co/7ns2XVH/eff1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wjyrnP6LpKf"
      },
      "source": [
        "## Rule generation\r\n",
        "The confidence of a rule can be computed from supports since it's just a ration: for confidence-based pruning it's sufficient to know the supports of frequent itemsets: $$\\text{conf}(A \\to C) = \\frac{\\text{sup}(A \\to C)}{\\text{sup}(A)}$$\r\n",
        "\r\n",
        "Given a frequent itemset $L$ find all the non-empty subset $f \\in L$ such that the confidence of rule $f \\to (L - f)$ is not less than the minimum confidence setted by the experiment designer.\r\n",
        "If $|L| = k$ then there are $2^k-2$ candidate rules.\r\n",
        "\r\n",
        "*Note*: $L \\to \\emptyset$ and $\\emptyset \\to L$ can be ignored.\r\n",
        "\r\n",
        "In general confidence is not anti-monotonic (like support), i.e. $\\text{conf}(ABC \\to D)$ can be larger or smaller than $\\text{conf}(AB \\to D)$.\r\n",
        "\r\n",
        "Nevertheless, confidence of rules generated from the same itemset is anti-monotone with respect to the number of times on the RHS of the rule (it decreases when we move an item from LHS to RHS of the rule), i.e. $\\text{conf}(ABC \\to D) \\ge \\text{conf}(AB \\to CD) \\ge \\text{conf}(A \\to BCD)  $.\r\n",
        "\r\n",
        "![](https://i.ibb.co/QKVBxsJ/pruning.png)\r\n",
        "\r\n",
        "In apriori algorithm a candidate rule is generated by merging two rules that share the same prefix in the rule consequent, i.e. joining $CD \\to AB$ and $BD \\to AC$ would produce the candidate rule $D \\to ABC$.\r\n",
        "\r\n",
        "The rule $D \\to ABC$ would be pruned if its subset $AD \\to BC$ doesn't have high confidence.\r\n",
        "\r\n",
        "Many real datasets have skewed support distribution:\r\n",
        "* If the $\\text{minsup}$ is set too high we could miss itemsets involving rare items (i.e. expensive products);\r\n",
        "* If the $\\text{minsup}$ is set too low the number of itemsets could be too large and computational expensive.\r\n",
        "\r\n",
        "Using a single minimum support threshold may not be effective.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_FtBEVHVfFn"
      },
      "source": [
        "### Multiple minimum support\r\n",
        "Let's consider $\\text{MS}(i)$ as minimum support for item $i$.\r\n",
        "Now support is no longer anti-monotonic (**bad**).\r\n",
        "\r\n",
        "The algorithm it's the same of the apriori but with some changes.\r\n",
        "\r\n",
        "1. Order the items according their minimum support in ascending order;\r\n",
        "\r\n",
        "Let be:\r\n",
        "* $L_1$: set of frequent items;\r\n",
        "* $F_1$: set of items whose support is $\\text{MS}(1)=\\min_i \\bigl(\\text{MS}(i)\\bigr)$;\r\n",
        "* $C_2$: candidate itemsets of size $2$ that is generated from $F_1$ instead of $L_1$.\r\n",
        "\r\n",
        "2. In traditional apriori a candidate $(k+1)$-itemset is generated by merging two frequent itemsets of size $k$ and the candidate it's pruned if it contains any infrequent subsets of size $k$.\r\n",
        "In the modified apriori the candidate it's pruned only if subset contains the first item.\r\n",
        "\r\n",
        "*Example*: the candidate is $\\{ \\text{Broccoli}, \\text{Coke}, \\text{Milk} \\}$ (ordered according their minimum support in ascending order).\r\n",
        "We found out that $\\{ \\text{Broccoli}, \\text{Coke} \\}$ and $\\{ \\text{Broccoli}, \\text{Milk} \\}$ are frequent, while $\\{ \\text{Coke}, \\text{Milk} \\}$ is infrequent. By the way, the candidate is not pruned since $ \\{ \\text{Coke}, \\text{Milk} \\}$ doesn't contain the first element $\\text{Broccoli}$.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9apZMcj4MBgZ"
      },
      "source": [
        "## Pattern evaluation\r\n",
        "Association rule algorithms tend to produce too many rules: many of them are uninteresting or redundant.\r\n",
        "\r\n",
        "**Redundancy**: $\\{ A,B,C \\} \\to \\{ D \\}$ and $\\{ A,B \\} \\to \\{ D \\}$ if they have same support and confidence.\r\n",
        "\r\n",
        "Interestingness measures can be used to prune or rank the derived patterns.\r\n",
        "\r\n",
        "Given a rule $A \\to C$ the information needed to compute *rule interestingness* can be obtained from a contingency table. \r\n",
        "The elements of the contingency table are the basis for most of the interestigness measure.\r\n",
        "\r\n",
        "![](https://i.ibb.co/XzBgmTx/photo-2021-01-06-15-57-56.jpg)\r\n",
        "\r\n",
        "Confidence can be misleading. *Example*:\r\n",
        "![](https://i.ibb.co/kym0X8M/photo-2021-01-06-16-00-27.jpg)\r\n",
        "\r\n",
        "We have:\r\n",
        "* $\\text{conf}(\\text{Tea} \\to \\text{Coffe}) = \\frac{\\text{sup}(\\text{Tea, Coffee})}{\\text{sup}(\\text{Tea})} = \\frac{15}{20} = 0.75$;\r\n",
        "* $P(\\text{Coffee}) = \\frac{90}{100} = 0.9$;\r\n",
        "* $P(\\text{Coffee}|\\overline{\\text{Tea}}) = \\frac{75}{80} = 0.9375$;\r\n",
        "\r\n",
        "So, despite the high confidence of $\\text{Tea} \\to \\text{Coffe}$ the absence of Tea increases the probability of Coffe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ndqNirBPJ7-"
      },
      "source": [
        "### Statistical independece (no correlation)\r\n",
        "* $P(S \\land B)=P(S)*P(B) \\to$ Statistical independece;\r\n",
        "* $P(S \\land B)>P(S)*P(B) \\to$ Positively correlated;\r\n",
        "* $P(S \\land B)<P(S)*P(B) \\to$ Negatively correlated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZcDJLQHPhwk"
      },
      "source": [
        "### Statisitical-based measure\r\n",
        "Are measures that take into account the deviation from statistical independence.\r\n",
        "\r\n",
        "**Lift** $$\\text{lift}(A \\to C) = \\frac{\\text{conf}(A \\to C)}{\\text{sup}(C)}=\\frac{P(A,C)}{P(A)*P(C)}$$\r\n",
        "* Evaluates $1$ for independence;\r\n",
        "* Insensitive to rule direction;\r\n",
        "* Ratio of true cases with respect to independence.\r\n",
        "\r\n",
        "**Leverage** $$\\text{leve}(A \\to C)=P(A,C) - P(A)*P(C) = \\text{sup}(A \\cup C) - \\text{sup}(A)*\\text{sup}(C)$$\r\n",
        "* Evaluates $0$ for independence;\r\n",
        "* Insensitive to rule direction;\r\n",
        "* Number of additional cases with respect to independence.\r\n",
        "\r\n",
        "**Conviction** or novelty $$\\text{conv}(A \\to C)=\\frac{1-\\text{sup}(C)}{1-\\text{conf}(A \\to C)} = \\frac{P(A)(1-P(C))}{P(A)-P(A,C)}$$\r\n",
        "* It's infinite if the rule is always true;\r\n",
        "* Sensitive to rule direction;\r\n",
        "* It's the ratio of the expected frequency that $A$ occurs without $C$ if $A$ and $C$ were independent divided by the observed frequency of incorrect predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLpZNwVTS5Z4"
      },
      "source": [
        "### Intuition about measures\r\n",
        "* Higher support $\\to$ Rule applies to more records\r\n",
        "* Higher confidence $\\to$ The chance that the rule is true for some record is higher;\r\n",
        "* Higher lift $\\to$ The chance that the rule is just a coincidence is lower;\r\n",
        "* Higher conviction $\\to$ The rule is violated less often that it would be if the antecedent and the consequent were independent.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5cErjsdS7P2"
      },
      "source": [
        "### Properties of a good measure (Piatetsky-Shapiro)\r\n",
        "1. $M(A,B)=0$ (or $1$) if $A$ and $B$ are statistically independent;\r\n",
        "1. $M(A,B)$ increases monotonically with $P(A,B)$ with fixed $P(A)$ and $P(B)$;\r\n",
        "1. $M(A,B)$ decreases monotonically with $P(A)$ (or $P(B)$) with fixed $P(A,B)$ and $P(B)$ (or $P(A)$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAjQqcYvT3MU"
      },
      "source": [
        "## Multidimensional association rules\r\n",
        "**Mono-dimensional** or intra-attribute: items $A$, $B$ and $C$ are together in a transaction.\r\n",
        "\r\n",
        "**Multi-dimensional** or inter-attribute: attribute $A$ has value $a$, attribute $B$ has value $b$ and attribute $C$ has value $c$ in a\r\n",
        "tuple.\r\n",
        "\r\n",
        "Most software packages for association rules discovery do not deal with\r\n",
        "quantitative attributes.\r\n",
        "\r\n",
        "**Discretization**: possibly *equifrequency* or with monoâ€“dimensional clustering, for optimal covering of the original value domains.\r\n",
        "\r\n",
        "Association rules can involve items at different levels.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhdWHaVoU8lE"
      },
      "source": [
        "## Multilevel association rules\r\n",
        "A real database can include tens of thousands of distinct items.\r\n",
        "Frequently it's necessary to find a tradeoff between general and detailed reasoning (choose the right level of abstraction).\r\n",
        "\r\n",
        "A common background knowledge is the organization of the items into a **hierarchy of concepts** since it can be easily coded in the transactions and it can help the choice of the right level of abstraction.\r\n",
        "\r\n",
        "![](https://i.ibb.co/zZMkxKb/hierachy.png)\r\n",
        "\r\n",
        "* From specialized to general the support of rules increases (in general) and new rules can become interesting;\r\n",
        "* From general to specialized the support of rules decreases (in general) and can go under the threshold.\r\n",
        "\r\n",
        "A level change can influence the confidence in any direction. If the specialized rules ha approximately the same confidence as the general one then it's redundant.\r\n",
        "\r\n",
        "**Mining**: look for frequent itemsets at each level of abstraction, top down. Each level requires a new run of the rule discovery algorithm, decreasing the support threshold in lower levels."
      ]
    }
  ]
}