{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_classification_part1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN84lIKzmupxUwsuxyYCuLi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rPN0-wPEQKWP"},"source":["# **Classification - Part I**\r\n","\r\n","With the term classification we always mean *supervised classification*.\r\n","\r\n","**The classification problem**: the dataset contains $N$ *individuals* described by $D$ *attribute values*: one of these attributes is the *class*, that allows a finite number $C$ of different values.\r\n","\r\n","The goal is to learn how to guess the value of the $D$-th attribute for individuals that have not been examined by the experts, so **learn a classification model**.\r\n","\r\n","**Classification model**: algorithm that, given an individual for which class is unknown, compute the class.\r\n","The algorithm is parametrized to optimize the results for the specific problem.\r\n","\r\n","To develop a classfication model:\r\n","* Choose the learning algorithm;\r\n","* Let the algorithm learn its parametrization;\r\n","* Asses the quality of the classification model, since it will be used at run-time for classification.\r\n","\r\n","There are two main flavours for classifications:\r\n","* **Crisp**: the classifier assigns one label;\r\n","* **Probabilistic**: the classifier assigns a probability for each possible labels.\r\n","\r\n","## A general workflow for classification\r\n","**Step 1**: learning the model for the given set of classes.\r\n","* A training set, obtained by a random process, is available;\r\n","* The training set contains a certain number of individuals and for each the class label is available.\r\n","\r\n","Note: the training set should be representative as much as possible.\r\n","\r\n","**Step 2**: estimate the accuracy of the model.\r\n","* A test set is available;\r\n","* The test set contains a certain number of individuals and for each the class label is available too;\r\n","* The model is run to assign the labels to each individual of the test set;\r\n","* The labels of the test set and the ones obtained by the classifier are compared and the accuracy estimated.\r\n","\r\n","**Step 3**: the model is used run-time to label new individuals. \r\n","\r\n","Note: it's possible that after the labelling the true ones becomes available, so that the true accuracy can be compared to the estimated one.\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"5k5gdyozYW7P"},"source":["# Classification with decision trees (C4.5 algorithm)\r\n","A run-time classifier (not learner) structured as a decision tree, hence a tree-shaped set of tests. The decision tree has inner nodes (decisions), leaf nodes (predicitons) and the root.\r\n","\r\n","## General description of the learner\r\n","Given a set $\\varepsilon$ of individuals for which the class is known, grow a tree as follows:\r\n","* If all the elements belong to class $c$ or $\\varepsilon$ is small enough, generate a leaf node with label $c$;\r\n","* Otherwise (if we can't generate a leaf):\r\n","> 1. Choose a test based on a single attribute with two or more outcomes;\r\n","  1. Make this test the root of a tree with one branch for each of the outcomes of the test;\r\n","  1. Partition $\\varepsilon$ into subsets corresponding to the outcomes and apply recursively the procedure to these subsets.\r\n","\r\n","Open questions:\r\n","1. Which attribute should we test? *We need an indicator to find out more interesting ones;*\r\n","1. Which kind of test? *Binary, multi-way, etc... (depends also on the domains of the attributes)*;\r\n","1. What $\\varepsilon$ small enough means? *Hyperparameter, we could use different thresholds*.\r\n","\r\n","**Contingency tables**: $k$-dimensional generalization of histograms (frequency counter).\r\n","\r\n","Our goal is to design an algorithm that find interesting patterns for forecasting, distinguish real pattern from illusions (choose useful patterns).\r\n","There are several methods to evaluate how much a pattern is interesting, one of them is the **[entropy](https://it.wikipedia.org/wiki/Entropia_(teoria_dell%27informazione))**, based on information theory."]},{"cell_type":"markdown","metadata":{"id":"M0VpVN04o_lt"},"source":["## Entropy and Information Gain\r\n","Given a source $X$ with $V$ possible values with probablity distribution: $$P(v_1)=p_1, P(v_2)=p_2, \\dots, P(v_V)=p_V$$ the best coding allow the transmission with an average number of bits given by: $$H(X)= -\\sum_{j} p_j \\log_{2}(p_j)$$ where $H(X)$ is the entropy of the information source $X$.\r\n","\r\n","High entropy means that the probabilities are mostly similar (flat histogram), low entropy means that some symbols have much higher probablity (histogram with peaks).\r\n","\r\n","**Higher number of allowed symbols gives higher entropy.**\r\n","\r\n","Example: in a binary source with symbol probabilities of $p$ and $(1-p)$, when $p$ is $0$ or $1$ the entropy goes to $0$.\r\n","\r\n","**Conditional specific entropy**: give an insight on attribute $Y$ knowing attribute $X$ (correlation, a sort of entropy filter) $$H(Y|X=v)$$\r\n","\r\n","**Conditional entropy**: weighted average of the conditional specific entropy of $Y$ with respect to the values of $X$ $$H(Y|X) = \\sum_{j} P(X=v_j)*H(Y|X=v_j)$$ It's the average number of bits necessary to transmit the value of $Y$ if both ends knows the values of $X$. \r\n","\r\n","**Information gain**: the amount of insight that $X$ provides to forecast the values of $Y$ $$IG(Y|X)=H(Y)-H(Y|X)$$ It's the average number of bits that can be saved to transmit the value of $Y$ if both ends knows the values of $X$. \r\n","\r\n","If $IG(Y|X)$ is low we can say that $X$ does not affect $Y$, so it won't be an interesting expansion."]},{"cell_type":"markdown","metadata":{"id":"NDAQKAP2pDZ4"},"source":["\r\n","## Back to decision trees\r\n","Calculate the information gain for each attribute.\r\n","\r\n","**One-stump decision**: choose attribute giving the highest $IG$, partition the dataset according to the chosen attribute and choose as class label of each partition the majority.\r\n","\r\n","**Recursive step**: build a new tree starting from each subset where the minority is non-empty (no unanimity), with another attribute.\r\n","\r\n","The generation stops when there is no more possibility of recursion: no more attribute or the minority is empty.\r\n"]},{"cell_type":"markdown","metadata":{"id":"HpbVZXE4pLOM"},"source":["## Errors and overfitting\r\n","The supervised data is partitioned in two sets:\r\n","* Training set, used to generate the model;\r\n","* Test set, used to compute the test set error with the generated model.\r\n","\r\n","**Training set error**: discordances between true label of the training set and the ones forecasted by the decision tree on the training set. \r\n","It can be non-zero due to the limits of the decision trees in general.\r\n","It's the error we make on the data used to generate the classification model.\r\n","It's the lower limit of the error we can expect when classifying new data, we're interested on an upper bound.\r\n","\r\n","**Test set error**: indicative of the expected behaviour with new data. Additional statistic reasoning can be used to infer error bounds.\r\n","\r\n","**Overfitting**: overfitting happens when the learning is affected by noise. When a learning algorithm is affected by noise, the performance on the test set is much worse than the one on the training set.\r\n","\r\n","More formally, a decision tree is a hypothesis of the relationship between the predictor attributes and the class. The hypotesis $h$ overfits (learns also the noise) the training set if there is an alternative hypothesis $h'$ such that:\r\n","$$error_{train}(h)<error_{train}(h')$$\r\n","$$error_{\\varepsilon}(h)>error_{\\varepsilon}(h')$$ \r\n","\r\n","### Causes of overfitting\r\n","1. Presence of noise: individuals can have bad values in the predicting attributes and/or in the class label;\r\n","1. Lack of representative instances: some situations of the real world can be under-represented or not at all.\r\n","\r\n","A good hypotesis has a low generalization error.\r\n","\r\n","**Ockham's razor**\r\n","> \"Everything should be made as simple as possible, but not simpler\"\r\n","\r\n","* All other thing being equal, simple theories are preferable complex ones; \r\n","* Long hypotesis that fits the data is more likely to be a coincidence;\r\n","* **Pruning** a decision tree is a way to simplify it."]},{"cell_type":"markdown","metadata":{"id":"HE9_DLqB2TCY"},"source":["## Pruning\r\n","In decision trees, at higher levels we have info (truth) and at lower levels we have noise.\r\n","* **Pre-pruning**: early stop of the tree growth, before it perfectly classifies the training set;\r\n","* **Post-pruning**: build a complete tree, then prune some portions according to some criteria. Usually preferred since it's not easy to estimate when to stop the growing of the tree.\r\n","\r\n","### Post-pruning criteria\r\n","* **Validation set**: use a distinct supervised dataset to evaluate the effect of post-pruning nodes from the tree;\r\n","* **Statistical pruning**: statistical test to estimate if pruning a particular node is likely to produce an improvement;\r\n","* **Minimum description lenght principle**: use an explicit measure of complexity for encoding the training set and the decision tree\r\n","$$\\min \\text{size}[\\text{size}(\\text{tree}) + \\text{size}(\\text{missclassification}(\\text{tree}, \\text{training set}))]$$\r\n","![](https://i.ibb.co/7JJ1qWk/photo-2020-12-28-15-58-37.jpg)\r\n","\r\n","### Validation set\r\n","The supervised data is partitioned in three independent sets:\r\n","* **Training set**: basis to build the model;\r\n","* **Validation set**: the model is tuned (pruned) to minimize the error;\r\n","* **Test set**: asses final expected error.\r\n","\r\n","### Statistical pruning\r\n","From inferential statistic, it's significance testing.\r\n","* **Error estimation**: does the pruning reduce the maximum error expected?\r\n","* **Significance testing**: is the contribution of a node compatible with a random effect?\r\n","\r\n","It's the most used in practice.\r\n","\r\n","### Minimum description lenght (MDL)\r\n","The learning process produce a theory on a set of data.\r\n","The theory is used to predict values for new data and can make errors.\r\n","The theory can be encoded, and errors as well as exceptions to the theory. According to the MDL principle a theory with shorter description is preferable.\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"jUEIbdjR6i5W"},"source":["## Decision tree learning algorithm\r\n","The base case uses greedy strategy: in each step chooses a local optimum and never goes back to try a different sequence of choices.\r\n","The options are:\r\n","1. **Specification of the test on the chosen attribute**: it depends on the value domain of the attribute:\r\n","> * If the domain of the attribute is discrete with $V$ values the split generates $V$ nodes; \r\n","  * If the domain is continuous with $V$ values the split in $V$ node is infeasible since the high number of branches would generate very small subsets and the significance would decrease rapidly.\r\n","  * **Discretization**: the continuous domain is converted into a set of discrete values, according to some discretization technique;\r\n","  * **Binarization**: extreme case of discretization. The split point of the domain is set with a threshold.\r\n","  * The threshold-based split may deploy computational issues.\r\n","\r\n","1. **Choice of the attribute to split the dataset**: we're looking for the split generating the maximum purity. \r\n","A measure of purity is needed (information gain) or impurity (gini index, missclassification error):\r\n","> * A node with two classes in the same proportion has low purity;\r\n","  * A node with only one class has the highest purity.\r\n","3. **When to stop splitting** (pruning).\r\n","\r\n","### Impurity functions\r\n","**Gini index**: consider a node $p$ with $C_p$ classes.\r\n","Which is the frequency of the wrong classification in class $j$ given by a random assignment based only on the class frequencies in the current node?\r\n","For class $j$:\r\n","* Frequency $f_{p,j}$;\r\n","* Frequency of other classes $1-f_{p,j}$;\r\n","* Probability of the wrong assignment $f_{p,j}*(1-f_{p,j})$.\r\n","\r\n","The gini index is the total probability of wrong classification: $$\\sum_j f_{p,j}*(1-f_{p,j}) = \\sum_j f_{p,j} - \\sum_j f^2_{p,j} = 1 - \\sum_j f^2_{p,j}$$\r\n","\r\n","The range is from $0$ to $1 - \\frac{1}{C_p}$. \r\n","The minimum value is when all records belong to the same class, the maximum value is when all records are uniformly distribuited over all the classes.\r\n","\r\n","We choose to split giving the maximum reduction of the Gini index (the purity increases the most).\r\n","\r\n","**Missclassification error**: if a node is a leaf we find the highest label frequency. \r\n","This frequency is the accuracy of the node and his label is the output of the node. \r\n","The missclassification error is the complement to one of the accuracy.\r\n","The range is from $0$ to $1 - \\frac{1}{C_p}$. \r\n","The minimum value is when all records belong to the same class, the maximum value is when all records are uniformly distribuited over all the classes.\r\n","\r\n","The choice of the split is done in the same way as for the Gini index:\r\n","$$\\text{ME}(p)=1-\\max_{j} f_{p,j}$$\r\n","\r\n","![](https://i.ibb.co/VTtnKm0/photo-2020-12-28-16-54-47.jpg)\r\n","\r\n","* The behaviour of ME is linear, therefore an error in the frequency is completely transferred into the impurity computation;\r\n","* Entropy and Gini have varying derivative with minimum around the center: they're more robust to errors when the frequencies of the two classes are similar.\r\n"]},{"cell_type":"markdown","metadata":{"id":"cGhNBjzXE2Uu"},"source":["## Characteristics of DT induction\r\n","There are several variants depending on the strategy of constructions, strategy of partition and strategy of pruning, with tests based on linear combination of numeric attributes.\r\n","* Non-parametric approach to build classificator, doesn't require any assumption on the distributions of classes and attributes;\r\n","* Finding the best tree is NP-complete, heuristic algorithms allow to find sub-optimal solutions in reasonable times;\r\n","* The run-time use of the decision tree is extremely efficient, costs $\\mathcal{O}(h)$ where $h$ is the height of the tree;\r\n","* Robust to noise if overfitting managed;\r\n","* Redundant attributes are not a problem: in case of strong correlation between two attribute, if one is chosen for a split the other will never provide a good increment of purity and won't be chose;\r\n","* Nodea at high depth are easily irrelevant since cover a low number of samples;\r\n","\r\n","\r\n","### Complexity\r\n","* $N$ istances (data-points) and $D$ attributes in $\\varepsilon$. Tree height is $\\mathcal{O}(\\log N)$;\r\n","* Each level of the tree requires the consideration of all the dataset;\r\n","* Each node requires the consideration of all the attributes;\r\n","* Binary split of numeric attributes costs $\\mathcal{O}(N \\log N)$, but doesn't increment the complexity;\r\n","* Pruning requires consideration of each node, but they are at most $2N-1$;\r\n","* Pruning requires to consider globally all istances at each level, costs $\\mathcal{O}(N \\log N)$, which, once again, doesn't increment the complexity;\r\n","\r\n","Overall cost is $\\mathcal{O}(D N \\log N)$."]}]}
