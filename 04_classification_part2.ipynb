{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_classification_part2.ipynb","provenance":[],"collapsed_sections":["p_KSPqXoyJIh","2Xh_t8KWyNQ5","MTNFFfdDySwQ","XyWpNshVyZJo","coKBKHp4ydJv","d1NS3y5Pyjq6","ocTBpMup3NV5"],"authorship_tag":"ABX9TyOz1JQGIh9JtFfbjqvAAVFh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Qb2Ue1AhWqBr"},"source":["# **Classification - Part II**\r\n"]},{"cell_type":"markdown","metadata":{"id":"5fE7d3erXR0r"},"source":["## Model selection\r\n","Which of the available models, algorithms and parameters configurations is the best one?\r\n","\r\n","**Oil slick example**: detect oil slick from satellite images for early alarm.\r\n","Manual detection is expensive and slow, but with machine learning there are some problem such as:\r\n","* Scarcity of training data: oil spill are rare (fortunately);\r\n","* Unbalanced nature of data (skew classes): non-spills samples are predominant over the positive ones.\r\n","\r\n","An automatic hazard detection system has been developed and marketed with a manual pre-selection of images for final manual processing. \r\n","It has been necessary a **tradeoff between undetected spills and false alarms**.\r\n","The evaluation of performance guides the tradeoff.\r\n","\r\n","> In real life I can decide if the classificator has to be biased in one direction or the other (cost of error).\r\n","\r\n","In supervised learning the training set performance is overoptimistic, we need a lower bound for performance obtained by independent tests.\r\n","\r\n","Supervised data are usually scarce and we need to balance the use between train, validation and test.\r\n","\r\n","It's important to evaluate how much the theory fits the data and evaluate the cost generated by prediction errors. The evaluation is independent from the algorithm used to genereate the classifier.\r\n","\r\n","Empirically the more training data we use the best performance we should expect, since we are covering a larger situation. But we have already seen that in this way we caputre noise too (random changes). \r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"p_KSPqXoyJIh"},"source":["### Error estimation\r\n","Let's suppose that the test set is a good representation, on the average, of the entire dataset $\\varepsilon$.\r\n","The relationship between the training set and the dataset will be subject to **probabilistic variability**.\r\n","The evaluation can be either:\r\n","* **General**: the whole performance of the classifier;\r\n","* **Local**: the local performance of a component of the model (i.e. a node of the DT).\r\n","\r\n","If the test set error ratio is $x$, we should expect a runtime error $x \\pm \\sigma$ (**confidence interval**).\r\n","\r\n","Forecasting each element of the test set is like a Bernoulli process: good prediction is a success $S$, a bad prediction is an error, the same as $N$ indipendent binary random events of the same type.\r\n","\r\n","If $f=\\frac{S}{N}$ is the empirical frequency of error, it has a normal distribution around the true probability (with $N \\ge 30$).\r\n","We choose a **confidence level** $\\alpha$ such as the probability that the true frequency of success is below the pessimistic frequency that we will compute:\r\n","$$P(z_{\\alpha/2} \\le \\frac{f-p}{\\sqrt{p(1-p)/N}} \\le z_{{1-\\alpha}/2}) = 1 - \\alpha$$\r\n","Where:\r\n","* $z_{\\alpha/2}$ and $z_{{1-\\alpha}/2}$ are the tails of the gaussian;\r\n","* $p$ is the probability of error;\r\n","* $f$ is the probability of success.\r\n","\r\n","The tail $z$ depends on the desiderd confidence level $\\alpha$, it's the abscissa delimiting the area $1-\\alpha$ for a normal distribution ([Wilson score interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval)).\r\n","With a little of algebra:\r\n","$$p \\approx \\frac{1}{1 + \\frac{1}{N}z^2} \\Biggl[f + \\frac{1}{2N}z^2 \\pm \\sqrt{\\frac{1}{N}f(1-f)+\\frac{1}{4N^2}z^2}\\Biggr]$$\r\n","\r\n","![](https://i.ibb.co/5rsFzB0/photo-2020-12-29-09-02-42.jpg)\r\n","Increasing $N$ with empirical frequency of error $f$ constant, the uncertainty for $p$ narrows."]},{"cell_type":"markdown","metadata":{"id":"2Xh_t8KWyNQ5"},"source":["### Statistical pruning of the decision tree\r\n","The C4.5 strategy is:\r\n","* Consider a subtree near the leaves;\r\n","* Compute its maximum error $e_l$ as weighted sum of the maximum error of the leaves;\r\n","* Compute the maximum error $e_r$ of the root of the subtree transformed into leaf;\r\n","* Prune if $e_r \\le e_l$, hence if the upstream error is less than the downstream one (worsening).\r\n","\r\n","With pruning, the error frequency increases and the number of records as well, therefore the maximum error can decrease."]},{"cell_type":"markdown","metadata":{"id":"MTNFFfdDySwQ"},"source":["### Testing a classifier\r\n","The *error frequency* (sum of errors on any class, divided by the number of tested records) is the simplest indicator of the quality of a classifier.\r\n","From now on for simplicity we will consider *empirical error frequencies* since we can just measure it, in real cases *error frequency* is used but it's computation is heavier.\r\n","\r\n","Accuracy and others more sophisticated indicators are used to compare different classifiers or parameter settings and the cost of errors.\r\n","\r\n","**Hyperparameters**: every machine learning algorithm has one or more parameters that influence its behaviour (i.e. in decision tree are impurity and maximum depth). \r\n","Several train/test loops are in general necessary to find the best set of values.\r\n","Sometimes is necessary to find a tradeoff between optimization step and the quality of the result (the optimization can be expensive).\r\n","\r\n","In each step the data should be representative of the data that will be classifed run-time. \r\n","Several testing strategies (splitting) for optimization step exists:\r\n","* **Holdout**: a typical split ratio is $2:1$.\r\n","The split should be as random as possible, and the proportion between classes should be mantained (sampling with **stratification**).\r\n","> * Splitting data into *training set* and *test set*;\r\n","  * Splitting data into *training set*, *validation set* (we change hyperparameters, retrain and choose the best) and *test set* (totally independent, not used in training, represents the run-time situation).\r\n","* **Cross-validation**\r\n","> * Repeated tests with different splits (different kinds);\r\n","![](https://i.ibb.co/bHbCd0t/photo-2020-12-29-09-30-54.jpg)\r\n","  * $k$-fold: the training set is randomly partitioned into $k$ subsets (folds).\r\n","  There are $k$ iterations using one of the folds for test and others for training (more complexity).\r\n","  Results are then combined and final model is generated on the entire training set, since each record is used $k-1$ times for training and once for testing (optimal use of the supervised data).\r\n","  Tipically $k=10$;\r\n","  * Leave one out: extreme case of cross-validation where $k=N$, no random partitioning nor stratification. It's very expensive.\r\n","\r\n","**Bootstrap**: statistical sampling technique. Sampling $N$ records with replacement. Each record can be re-selected, others are never selected and will be used for test."]},{"cell_type":"markdown","metadata":{"id":"XyWpNshVyZJo"},"source":["### Performance measure of a classifier\r\n","**Confusion matrix**: let's consider for simplicity a binary prediction.\r\n","It's possible to define the so called confusion matrix, structured in the following way:\r\n","![](https://miro.medium.com/max/2102/1*fxiTNIgOyvAombPJx5KGeA.png)\r\n","We can define:\r\n","* **Success rate** or **accuracy** as $\\frac{TP+TN}{N_{\\text{test}}}$;\r\n","* **Error rate** as $1-\\text{Success rate}$.\r\n","\r\n","There are others performance indicator for a classifier such as: velocity, robustness, scalability, etc...\r\n","A classification error may have different consequences, depending on the class of the individual (sample).\r\n","For example, when forecasting an illness a false positive can be less dangerous than a false negative (unless cares or further examinations are dangeruous or invasive).\r\n","\r\n","There are others measure, such as:\r\n","* **Precision**: the rate of true positives among positive classifications $\\frac{TP}{TP+FP}$;\r\n","* **Recall** or **sensitivity**: the rate of positives I can catch $\\frac{TP}{TP+FN}$;\r\n","* **Specifity**: the rate of negatives that I can catch $\\frac{TN}{TN+FP}$;\r\n","* **Accuracy**: the weighted sum of sensitivity and specificity $\\text{acc}=\\text{sens}\\frac{\\text{pos}}{N}+\\text{spec}\\frac{\\text{neg}}{N}$;\r\n","* **F-measure**: armonic mean of precision and recall (maximum when they're equal) $F = 2 \\frac{\\text{prec} * \\text{rec}}{\\text{prec}+\\text{rec}}$.\r\n","\r\n","The confusion matrix may be extended to the multidimensional case (on the main diagonal there are correct predictions).\r\n","\r\n","It is useful to give also global definitions, which combine the values\r\n","of the single classes:\r\n","* **Macro**: the gloabl performance measure is more penalized if the\r\n","minority class has a bad performance;\r\n","* **Micro**: the performance measure of each class contributes at the\r\n","same extent to the global measure;\r\n","* **Weighted**: the performance measure of the majority class has a major influence in the global measure."]},{"cell_type":"markdown","metadata":{"id":"coKBKHp4ydJv"},"source":["### $k$ statistic\r\n","It's a parameter that evaluates the concordance between two classifications (i.e. predicted and true values).\r\n","Let be:\r\n","* Probability of concordance $Pr(c)=\\frac{TP_a + TP_b + TP_c}{N}$;\r\n","* Probability of random concordance $Pr(r)=\\frac{T_a+P_a + T_b*P_b + T_c*P_c}{N^2}$.\r\n","\r\n","Then, $k$ will be the ratio between the concordance exceeding rhe random component and the maximum surplus possible:\r\n","$$-1 \\le k = \\frac{Pr(c)-Pr(r)}{1-Pr(r)} \\le 1$$\r\n","\r\n","We will have:\r\n","* $1$ for perfect agreement;\r\n","* $0$ for total disagreement (rare);\r\n","* $0$ for random agreement.\r\n","![](https://i.ibb.co/yYW16Rr/photo-2020-12-29-14-46-47.jpg)"]},{"cell_type":"markdown","metadata":{"id":"d1NS3y5Pyjq6"},"source":["### The cost of errors\r\n","Our decisions are driven by predictions and bad predictions imply a cost.\r\n","An easy way to compute it's to compute the weighted cost of errors.\r\n","* Alternative 1: alterate the proportion of classes in the supervised data, duplicating the examples where the classification error is higher.\r\n","In this way the classifier will became more able to classify the classes where the cost is higher (we repeat train in these classes);\r\n","* Alternative 2: some learning algorithms allow to add weights to the istances."]},{"cell_type":"markdown","metadata":{"id":"s0j_il8ZzhRC"},"source":["## Evaluation of a probabilistic classifier\r\n","We've already seen the distinction between:\r\n","* **Crisp prediction**: gives label, immediate decision;\r\n","* **Probabilistic prediction**: gives probabilities of labels, soft prediction.\r\n","\r\n","The adequacy of on output rather than another depend on tha application domain: when an immediate decision is required a crisp classifier is necessary, when the classifier is part of a process that requires different steps a probabilistic classifier can be more appropriated.\r\n","\r\n","Crisp values sometimes hide probabilities, for example when a leaf has some counts for the minority classes. Since it quite common to have leaves with a small number of samples of minority classes, smoothing techniques are used to adjust probabilities.\r\n","\r\n","Probabilities can be converted into crisp values with different technqiues:\r\n","* Binary: a threshold fo positive classes is setted;\r\n","* Multiclass: output the class with maximum probability."]},{"cell_type":"markdown","metadata":{"id":"ocTBpMup3NV5"},"source":["### Lift chart\r\n","It's a chart used to evaluate various scenarios:\r\n","* Apply a probabilistic classification scheme;\r\n","* Sort all the classified elements for decreasing probability of the positive class;\r\n","* Make a $2$-dimensional chart with axes:\r\n","> * $x = \\text{sample size}$\r\n","  * $y = \\text{number of positives in sample}$\r\n","\r\n","Only the rank is important, not the specific probability. \r\n","![](https://www.bayesserver.com/docs/images/lift-chart.png)\r\n","* The blue straight line plot the number of positives obtained with a random choice of a sample of test data;\r\n","* The red curve plots all the classified elements for decreasing probability of the positive class (first the true positives);\r\n","* The orange curve is the perfect classifier.\r\n","\r\n","The larger the area between the two curves, the better is the classifier."]},{"cell_type":"markdown","metadata":{"id":"AahC53uh58VH"},"source":["### [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (receiver-operator characteristic)\r\n","\r\n","There is a tradeoff between hit rate and false alarm in a noisy channel.\r\n","The noise can be such that the recognition of the transmission is altered, in particular it alters the two levels according to a gaussian distribution.\r\n","The goal is to set a threshold in order to maximize tha above tradeoff.\r\n","With less noise the two gaussian are better separated:\r\n","![](https://i.ibb.co/30Sbv8Y/dvsvsvse.png)\r\n","\r\n","Moving the threshold towards right increases both the rate of true positives and false positives caught.\r\n","The area between the non-discrimination line and the ROC curve is a quality index of the line (the bigger the better).\r\n","The maximum area is the upper left triangle.\r\n","\r\n","A soft classifier can be converted into a crisp one setting a threshold. Varying the threshold the behaviour of the classifier changes by changing the ratio of $TP$ and $FP$.\r\n","Threshold steps allow to track the ROC curve."]},{"cell_type":"markdown","metadata":{"id":"6atxPeH687X9"},"source":["## From a binary classifier to multi-class classification\r\n","Several classifier generates a binary classification model.\r\n","There are two main approaches to deal with a multi-class classification:\r\n","* Transform the training algorithm and model (increase the size of the problem);\r\n","* Use a set of binary classifiers and combine the results (increase the number of problems)\r\n","> * One-vs-one strategies;\r\n","  * One-vs-all strategies.\r\n","\r\n","### One-vs-one strategy (OVO)\r\n","Consider all the possible $\\frac{C(C-1)}{2}$ pairs of classes and generate a binary classifier for each pair.\r\n","Each binary problem will consider only samples from the two selected classes.\r\n","\r\n","A prediction time is applied a voting scheme: an unseen example is submitted to all the classifiers and each winners receives a $+1$. The class with the highest value wins.\r\n","\r\n","### One-vs-all (OVA)\r\n","Consider $C$ binary problems where one class is the positive example and all the others are negatives.\r\n","Build $C$ binary classifiers and a prediction time apply a voting scheme: an unseen example is submitted to all the classifiers obtaining a confidence score. The confidences are combined and the class with the highest global score wins.\r\n","\r\n","OVO requires solving a higher number of problems, even if they are of smaller size. OVA it's intrinsically unbalanced ($1:C-1$)."]},{"cell_type":"markdown","metadata":{"id":"8w572z5gCSNy"},"source":["## Ensemble methods\r\n","The idea is to train a set of base classifier rather than a single one. \r\n","The final prediction is obtained taking votes of the base classifier.They tend to have better performances since their errors are uncorrelated and the ensemble is wrong only if the majority of the classifier is wrong (the error rate drops).\r\n","\r\n","### Manipulating the training set (filtering rows)\r\n","Data are resampled according to some sampling strategy.\r\n","* **Bagging**: repeatedly samples with replacement according to a uniform probability distribution;\r\n","* **Boosting**: iteratively changes the distribution of the training examples so that the base classifer focus on the examples that are harder to classify;\r\n","* **Adaboost**: the importance of each base classifier depends on its error rate (different voting schemes).\r\n","\r\n","### Manipulating input features (filtering columns)\r\n","Subset of input features can be chosen randomly or according to domain experts.\r\n","* **Random forest**: uses decision trees as base classifiers. Frequently produces very good results.\r\n","\r\n","### Manipulating class labels\r\n","Useful when the number of classes is high.\r\n","1. For each base classifier randomly partition class labels into two subsets and relabel the dataset;\r\n","1. Train a binary classifier with these two classes;\r\n","1. At testing time when a subset it's selected all the classes included receive a vote;\r\n","1. The class with the top score wins (error-correcting output coding)\r\n","\r\n","![](https://i.ibb.co/kSLbft3/photo-2020-12-29-16-27-52.jpg)"]}]}