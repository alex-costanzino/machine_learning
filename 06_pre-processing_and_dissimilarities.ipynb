{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_pre-processing_and_dissimilarities.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPiKt71GOMAZy5XTux/kKO9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vcbtKmxt2SYf"},"source":["# **Pre-processing and dissimilarities**\n","In real world data are dirty, noisy, huge and in part useless."]},{"cell_type":"markdown","metadata":{"id":"hjslVmwO2c9P"},"source":["## Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"QxX8KFDw6tbA"},"source":["### Aggregation\n","Combining two or more attributes into a single one for:\n","* Data reduction;\n","* Change of scale (i.e. cities aggregated into regions, states, countries, etc...);\n","* Stable data (reduced variability)."]},{"cell_type":"markdown","metadata":{"id":"QQYAPe2L6vqN"},"source":["\n","### Sampling\n","Processing or obtaining the entire dataset could be too expensive or time consuming. Using a sample will work almost as well as using the entire dataset if the sample is **representative** (has approximatively the same properties).\n","It comes in different flavours:\n","* **Simple random**: single random choice given a probability distribution;\n","* **With replacement**: repeated indipendent simple random extraction. Easier to implement and to be interpreted;\n","* **Without replacement**: repeated indipendent simple random extraction, but the extracted element is removed from the pupulation. Nearly equivalent with the one before if sample size is a small fraction of the dataset size.\n","* **Stratified**: split data according some criteria, then draw random samples from each partitions. It's used to maintain proportion between the classes.\n","\n","The choice of the sample size it's a tradeoff between data reduction and precision. We need to assess optimal sample size and sample significativity.\n","\n","The probability of sampling at least one element for each class is independent from the size of the dataset (depends on the sample size)."]},{"cell_type":"markdown","metadata":{"id":"1UwR77gy7Ij5"},"source":["### Dimensionality and dimensionality reduction\n","**Curse of dimensionality**: when dimensionality is very high the occupation of the space becomes very sparse (20 or more).\n","Discrimination on the basis of the distance becomes uneffective (concept of neighbourhood).\n","\n","There are some techinques for dimensionality reduction in order to: avoid the curse of dimensionality, reduce noise, reduce complexity and help visualization.\n","\n","**Principal component analysis (PCA)**: find projections that capture most of the data variation.\n","Find the eigenvectors of the covariance matrix, that define a new space.\n","The new dataset will have only the attributes which capture most of the data variation.\n","\n","**Singual values decompositions (SVD)**\n","\n","**Feature subset selection**: it's a local way to reduce dimensionality removing redundant or irrilevant attributes.\n","Different kinds:\n","* **Brute force**: try all possible subsets as input to data mining algorithm and measure effectiveness of the algorithm with the reduced dataset;\n","* **Embedded approach**: features selection occurs naturally as part of the mining algorithm (i.e. decision trees);\n","* **Filter approach**: feature are selected before the mining;\n","* **Wrapper approach**: the mining algorithm can choose the best set of attributes (like brute force but following some heuristic, not exhaustive search).\n","\n","**Feature creation**: new features can capture more effectively data characteristics (i.e. extraction, mapping)."]},{"cell_type":"markdown","metadata":{"id":"QEiXhDJSDCHm"},"source":["### Discretization\n","Some algorithms work better with categorical data and a smaller number of distinct values can let patterns emerge more clearly (there is less noise and randomness too).\n","* **Continuous to discrete**: thresholding (if just one it's binarization);\n","* **Discrete with many values to discrete with less values**: guided by domain knowledge.\n","\n","Example (unsupervised):\n","![](https://i.ibb.co/nMZMtQX/IAP-l.jpg)"]},{"cell_type":"markdown","metadata":{"id":"adENJPRwDKTK"},"source":["### Attribute transformation\n","Map the entire dataset of values to a new set according to a function. In general they change the distribution of values.\n","Some types:\n","* **Standardization**: translation with shrinking or stretching, don't change the distribution $x \\to \\frac{x-\\mu}{\\sigma}$;\n","* **Normalization (min/max)**: domains are mapped into standard ranges.\n","> * $x \\to \\frac{x-x_\\min}{x_\\max - x_\\min}$ maps to $[0,1]$;\n","  * $x \\to \\frac{x-{\\frac{x_\\max + x_\\min}{2}}}{\\frac{x_\\max + x_\\min}{2}}$ maps to $[-1,1]$."]},{"cell_type":"markdown","metadata":{"id":"7qvXE4SxEnzL"},"source":["## Similarity and dissimilarity\n","**Similarity**: measure of how alike two data objecs are. \n","Higher when objects are more alike.\n","Usually in range $[0,1]$.\n","\n","**Dissimilarity**: measure of differe two data objects are.\n","Lower when objects are more alike.\n","None standard range.\n","\n","Proximity refers to a similarity or dissimilarity.\n","\n","![](https://i.ibb.co/yVxBWZr/photo-2020-12-31-10-19-45.jpg)"]},{"cell_type":"markdown","metadata":{"id":"V4Fik8iZGhmL"},"source":["### Distances\n","**Euclidean distance $L_2$**: $D$ is the number of attributes, $p_d$ and $q_d$ are the $d$-th attributes of the data objects $p$ and $q$. Normalization is necessary if scales differs a lot.\n","$$\\text{dist}=\\sqrt{\\sum_{d=1}^{D}(p_d - q_d)^2}$$\n","\n","**Minkowski distance $L_r$**: $r$ is a parameter thar depends on the dataset or application.\n","$$\\text{dist}=\\Bigl({\\sum_{d=1}^{D}|p_d - q_d|^r}\\Bigr)^{\\frac{1}{r}}$$\n","It's the most general:\n","* If $r=1$ it's the $L_1$ norm, the so called *Manhattan distance*. Works better than euclidean in very high dimensional spaces;\n","* If $r=2$ it's the $L_2$ norm;\n","* If $r=\\infty$ it's the $L_\\infty$ norm, the so called *Chebyshev distance* or *supremum distance*. Considers only the dimensions where the difference is maximum. Provides a simplified evaluation disregarding the dimensions with lower differences.\n","$$\\text{dist}_\\infty=\\max_{d}|p_d-q_d|$$\n","\n","**Mahalanobis distance**: more sophisticated, considers the data distribution.\n","Decreases if, keeping the same euclidean distance, the segment connecting two points is stretched along a direction of greater veriation of data.\n","Described by [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of the data set:\n","$$\\sum_{ij}=\\frac{1}{N-1}(e_{ki}-\\overline{e_i})(e_{kj}-\\overline{e_j})$$\n","$$\\text{dist}_m = \\sqrt{(p-q){\\sum}^{-1}(p-q)^T}$$\n","\n","The Mahalnobis distance between two point is higher if the data are less distribuited in that direction.\n","\n","**Properties of distance (metric)**\n","* **Positive definiteness** $\\text{dist}(p,q) \\ge 0$ $\\forall p,q$ and $\\text{dist}(p,q) = 0$ iff $p=q$;\n","* **Symmetry**: $\\text{dist}(p,q) = \\text{dist}(q,p)$;\n","* **Triangle inequality** $\\text{dist}(p,q) \\le \\text{dist}(p,r) + \\text{dist}(r,q)$ $\\forall p,q,r$\n"]},{"cell_type":"markdown","metadata":{"id":"HFBqxdpNN8E3"},"source":["### Similarities\n","**Similarity in binary spaces**: consider:\n","* $M_{00}$ the number of attribues where $p=0$ and $p=0$;\n","* $M_{01}$ the number of attribues where $p=0$ and $p=1$;\n","* $M_{10}$ the number of attribues where $p=1$ and $p=0$;\n","* $M_{11}$ the number of attribues where $p=1$ and $p=1$;\n","\n","We can define:\n","* **Simple matching coefficient**: $\\text{SMC} = \\frac{M_{00}+M_{11}}{M_{00}+M_{01}+M_{10}+M_{11}}$ (number of matches over number of attributes);\n","* **Jaccard coefficient**: $\\text{JC} = \\frac{M_{11}}{M_{01}+M_{10}+M_{11}}$ (disregards negative matches).\n","\n","**Cosine similarity**: $\\cos(p,q) = \\frac{p \\cdot q}{||p|| \\cdot ||q||}$ (useful for positive values).\n","\n","**Extended Jaccard coefficient (Tanimoto)**: $T(p,q) = \\frac{pq}{{||p||}^2 + {||q||}^2 - pq}$\n","\n","**Properties of similarity**\n","* $\\text{sim}(p,q)=1$ if $p=q$;\n","* $\\text{sim}(p,q)=\\text{sim}(q,p)$."]},{"cell_type":"markdown","metadata":{"id":"EZ9OCaNtQy2U"},"source":["### How to choose the right proximity measure?\n","* If data are dense and continuous use a **metric measure**;\n","* If data are sparse and asymmetric use **similarity measure**."]},{"cell_type":"markdown","metadata":{"id":"HRoNr7kGRIKX"},"source":["## Correlation\n","Measure the linear relationship between a pair of attributes.\n","Start from $p=[p_1,\\dots,p_n]$ and $q=[q_1,\\dots,q_n]$, standardize them dividing by the $n$-th element and obtain $p'$ and $q'$.\n","Compute the dot product: $$\\text{corr}(p,q) = p' \\cdot q' $$\n","\n","Indipendent variables has zero correlation, but the inverse is not in general valid: zero correlation means the absence of linear relationship between the variables.\n","Positive values imply the positive linear relationship.\n","![](https://upload.wikimedia.org/wikipedia/commons/0/02/Correlation_examples.png)\n","\n","For nominal attribute I can't compute the dot product so I use the **symmetric uncertainty**, exploiting the entropy: $$U(p,q) = 2 \\frac{H(p) + H(q) - H(p,q)}{H(p) + H(q)}$$\n","\n","From a complete independence $U(p,q)=0$ to a complete biunivocal correspence $U(p,q)=1$.\n","When there is independence the joint entropy is the sum of the individual entropies.\n","When there is complete correspondence the individual entropies and the joint one are equal."]}]}