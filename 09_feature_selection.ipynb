{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09_feature_selection.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOy1zy9Ldxd8h1aWf2jH6cN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1HndYrnvrDEL"},"source":["# **Feature selection**\r\n","Features, attributes, columns, properties are all synonims for us.\r\n","\r\n","The significance of attributes for the purposes of data mining can vary highly:\r\n","* **Irrelevant alteration**: they can alter the results of some mining algorithms, in particular if there's no sufficient control of overfitting;\r\n","* **Redundancies**: some attributes can be strongly related to other useful attributes;\r\n","> * **Alteration**: some mining algorithms (i.e. Naive Bayes) are strongly influenced by strong correlation (since they're based on probabilities).\r\n","* **Confunding**: some attributes can be misleading like having **hidden effect** on the outcome variable;\r\n","> **Mixed effect**: i.e. one attribute could be strongly related to the class in 65% of the cases and random in the other cases.\r\n","\r\n","**Why feature selection?** Sometimes less is better. It may:\r\n","* Enables the machine learning algorithm to train faster;\r\n","* Reduces the complexity of a model, making it easier to interpret;\r\n","* Improves the accuracy of a model;\r\n","* Reduces overfitting.\r\n","\r\n","Note: it may be the case that a specific selection action obtain only one of the\r\n","above effects."]},{"cell_type":"markdown","metadata":{"id":"ZDUMLl-auDcY"},"source":["### Supervised or not?\r\n","**Unsupervised**: lots of methods available (i.e. clustering, feature trasformation such as PCA);\r\n","**Supervised**: consider the relationship between each attribute and the class (i.e. filter methods, scheme-dependent and scheme-independent selection, wrapper methods, embedded methods such as Lasso and Ridge regression)."]},{"cell_type":"markdown","metadata":{"id":"7g8Rp53fvGt9"},"source":["## Filter methods\r\n","The assesments is based on general characteristic of data.\r\n","It selects the subset of attributes indipendentely from the mining model that will be used.\r\n","\r\n","* **Pearson's correlation**: quantifies the amount of linea dependence between two variables;\r\n","* **LDA - Linear discriminant analysis**: find a linear combination of features that characterizes or separates two or more classes;\r\n","* **ANOVA - Analysis of variance**: similar to LDA but uses independet categorical features and a dependent continuous feature; \r\n","* **Chi-Square**: statistical test applied to the groups of categorical feature to evaluate the likelihood of correlation or association using frequency distribution.\r\n","\r\n","![](https://i.ibb.co/gr4F0t1/zin.jpg)"]},{"cell_type":"markdown","metadata":{"id":"IRgWLzKaSDAv"},"source":["## Wrapper methods\r\n","Try to use a subeset of features and train a model using them, adding or removing features from the subset: basically a greedy search problem (high computational cost).\r\n"]},{"cell_type":"markdown","metadata":{"id":"SkGkQVL_WJxB"},"source":["### RFE - Recursive feature elimination\r\n","It's a feature ranking with recursive feature elimination.\r\n","Uses an external estimator to assign weights to features.\r\n","Considers smaller and smaller sets of features\r\n","\r\n","The estimator is trained on the initial set and the importance of each feature is obtained: the lest important ones are pruned.\r\n","\r\n","Repeat and stops when the desired number of features is reached."]},{"cell_type":"markdown","metadata":{"id":"D4kyl4GnSX7s"},"source":["### Difference between filter and wrapper methods\r\n","\r\n","Filter | Wrapper\r\n","--- | ---\r\n","Measure the relevance of features by their correlation with the dependent variable | Measure the usefullness of features actually training a model (stronger)\r\n","Faster (no training) | Slower and expensive\r\n","Statistical methods for evaluation | Cross-validation for evaluation\r\n","Might fail (suboptimal) | Always find best subset\r\n","Less prone to overfit | More prone to overfit"]},{"cell_type":"markdown","metadata":{"id":"OnDp4sC8Thw8"},"source":["## Dimensionality reduction\r\n","Instead of considering which subset of attributes is to be ignored it is possible to map the dataset into a new space with fewer attributes (i.e. PCA)."]},{"cell_type":"markdown","metadata":{"id":"rbBI-pQJTsLV"},"source":["### PCA - Principal component analysis\r\n","![](https://i.ibb.co/5RxHrqJ/photo-2021-01-03-18-33-47.jpg)\r\n","\r\n","The covariance matrix is positive semidefinite, eigenvalues are positive and sorted in decreasing order, while eigenvectors are sorted according to the eigenvalue order."]},{"cell_type":"markdown","metadata":{"id":"QUbq0kBYUOC0"},"source":["### MDS - Multi-dimensional scaling\r\n","It's a presentation technique (even just to have a visual representation of the data).\r\n","\r\n","Starting from the distances among the elements of the dataset, fits the projection of the element into a $m$-dimensional space in such way that the distances among the elements are preserved.\r\n","\r\n","Exists for metric and non-metric spaces as well."]},{"cell_type":"markdown","metadata":{"id":"tWaYxBtOVD3_"},"source":["## Univariate feature selection\r\n","Select the best set of features based on univariate statistical test:\r\n","* Consider the original set of features and the target;\r\n","* For each feature, return a score and a p-value;\r\n","* Then:\r\n","> * **Select k-best**: remove all but the $k$ highest scoring features;\r\n","  * **Select percentile**: removes all but a user-specified highest scoring percentage of features.\r\n","\r\n","**Scoring function**: used by the feature selector to evaluate how much a feature is useful to predict the target (i.e. **Mutual information**, generalization of information gain)."]}]}
